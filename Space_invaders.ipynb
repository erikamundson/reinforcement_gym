{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 73, Reward: 40.0, avg loss: -0.14784, eps: 0.999\n",
      "Episode: 74, Reward: 90.0, avg loss: 0.03072, eps: 0.998\n",
      "Episode: 75, Reward: 210.0, avg loss: 0.03526, eps: 0.997\n",
      "Episode: 76, Reward: 180.0, avg loss: 0.03008, eps: 0.996\n",
      "Episode: 77, Reward: 65.0, avg loss: 0.02978, eps: 0.995\n",
      "Episode: 78, Reward: 315.0, avg loss: 0.03364, eps: 0.993\n",
      "Episode: 79, Reward: 105.0, avg loss: 0.03575, eps: 0.992\n",
      "Episode: 80, Reward: 180.0, avg loss: 0.03280, eps: 0.991\n",
      "Episode: 81, Reward: 110.0, avg loss: 0.03193, eps: 0.990\n",
      "Episode: 82, Reward: 215.0, avg loss: 0.03367, eps: 0.988\n",
      "Episode: 83, Reward: 20.0, avg loss: 0.04044, eps: 0.987\n",
      "Episode: 84, Reward: 120.0, avg loss: 0.03422, eps: 0.986\n",
      "Episode: 85, Reward: 270.0, avg loss: 0.03332, eps: 0.985\n",
      "Episode: 86, Reward: 110.0, avg loss: 0.03255, eps: 0.984\n",
      "Episode: 87, Reward: 35.0, avg loss: 0.02913, eps: 0.983\n",
      "Episode: 88, Reward: 105.0, avg loss: 0.03381, eps: 0.982\n",
      "Episode: 89, Reward: 20.0, avg loss: 0.03947, eps: 0.981\n",
      "Episode: 90, Reward: 185.0, avg loss: 0.03491, eps: 0.979\n",
      "Episode: 91, Reward: 70.0, avg loss: 0.03821, eps: 0.979\n",
      "Episode: 92, Reward: 210.0, avg loss: 0.03636, eps: 0.977\n",
      "Episode: 93, Reward: 60.0, avg loss: 0.03782, eps: 0.976\n",
      "Episode: 94, Reward: 50.0, avg loss: 0.03627, eps: 0.976\n",
      "Episode: 95, Reward: 75.0, avg loss: 0.04083, eps: 0.975\n",
      "Episode: 96, Reward: 65.0, avg loss: 0.04519, eps: 0.974\n",
      "Episode: 97, Reward: 225.0, avg loss: 0.03871, eps: 0.972\n",
      "Episode: 98, Reward: 210.0, avg loss: 0.03739, eps: 0.971\n",
      "Episode: 99, Reward: 55.0, avg loss: 0.04098, eps: 0.970\n",
      "Episode: 100, Reward: 125.0, avg loss: 0.03942, eps: 0.968\n",
      "Episode: 101, Reward: 175.0, avg loss: 0.03987, eps: 0.967\n",
      "Episode: 102, Reward: 120.0, avg loss: 0.03499, eps: 0.966\n",
      "Episode: 103, Reward: 260.0, avg loss: 0.03574, eps: 0.964\n",
      "Episode: 104, Reward: 120.0, avg loss: 0.03777, eps: 0.963\n",
      "Episode: 105, Reward: 240.0, avg loss: 0.03451, eps: 0.961\n",
      "Episode: 106, Reward: 210.0, avg loss: 0.03854, eps: 0.959\n",
      "Episode: 107, Reward: 305.0, avg loss: 0.03750, eps: 0.957\n",
      "Episode: 108, Reward: 155.0, avg loss: 0.03638, eps: 0.956\n",
      "Episode: 109, Reward: 185.0, avg loss: 0.04186, eps: 0.954\n",
      "Episode: 110, Reward: 110.0, avg loss: 0.03618, eps: 0.953\n",
      "Episode: 111, Reward: 75.0, avg loss: 0.03394, eps: 0.952\n",
      "Episode: 112, Reward: 135.0, avg loss: 0.03573, eps: 0.951\n",
      "Episode: 113, Reward: 215.0, avg loss: 0.03402, eps: 0.949\n",
      "Episode: 114, Reward: 80.0, avg loss: 0.03802, eps: 0.948\n",
      "Episode: 115, Reward: 120.0, avg loss: 0.03779, eps: 0.947\n",
      "Episode: 116, Reward: 210.0, avg loss: 0.03741, eps: 0.946\n",
      "Episode: 117, Reward: 230.0, avg loss: 0.03797, eps: 0.945\n",
      "Episode: 118, Reward: 180.0, avg loss: 0.03742, eps: 0.943\n",
      "Episode: 119, Reward: 135.0, avg loss: 0.03726, eps: 0.942\n",
      "Episode: 120, Reward: 330.0, avg loss: 0.04136, eps: 0.940\n",
      "Episode: 121, Reward: 110.0, avg loss: 0.03952, eps: 0.939\n",
      "Episode: 122, Reward: 130.0, avg loss: 0.03665, eps: 0.938\n",
      "Episode: 123, Reward: 40.0, avg loss: 0.03752, eps: 0.936\n",
      "Episode: 124, Reward: 75.0, avg loss: 0.03611, eps: 0.935\n",
      "Episode: 125, Reward: 460.0, avg loss: 0.03803, eps: 0.933\n",
      "Episode: 126, Reward: 210.0, avg loss: 0.03803, eps: 0.932\n",
      "Episode: 127, Reward: 155.0, avg loss: 0.03955, eps: 0.931\n",
      "Episode: 128, Reward: 70.0, avg loss: 0.03592, eps: 0.929\n",
      "Episode: 129, Reward: 110.0, avg loss: 0.04067, eps: 0.929\n",
      "Episode: 130, Reward: 50.0, avg loss: 0.04027, eps: 0.928\n",
      "Episode: 131, Reward: 80.0, avg loss: 0.04024, eps: 0.927\n",
      "Episode: 132, Reward: 105.0, avg loss: 0.03258, eps: 0.926\n",
      "Episode: 133, Reward: 135.0, avg loss: 0.04073, eps: 0.925\n",
      "Episode: 134, Reward: 25.0, avg loss: 0.04110, eps: 0.923\n",
      "Episode: 135, Reward: 110.0, avg loss: 0.04536, eps: 0.922\n",
      "Episode: 136, Reward: 325.0, avg loss: 0.04222, eps: 0.921\n",
      "Episode: 137, Reward: 145.0, avg loss: 0.04380, eps: 0.919\n",
      "Episode: 138, Reward: 35.0, avg loss: 0.04044, eps: 0.919\n",
      "Episode: 139, Reward: 105.0, avg loss: 0.04620, eps: 0.917\n",
      "Episode: 140, Reward: 105.0, avg loss: 0.03867, eps: 0.916\n",
      "Episode: 141, Reward: 430.0, avg loss: 0.04492, eps: 0.914\n",
      "Episode: 142, Reward: 55.0, avg loss: 0.04223, eps: 0.913\n",
      "Episode: 143, Reward: 350.0, avg loss: 0.04435, eps: 0.912\n",
      "Episode: 144, Reward: 120.0, avg loss: 0.04019, eps: 0.910\n",
      "Episode: 145, Reward: 160.0, avg loss: 0.03918, eps: 0.909\n",
      "Episode: 146, Reward: 105.0, avg loss: 0.03937, eps: 0.908\n",
      "Episode: 147, Reward: 265.0, avg loss: 0.04127, eps: 0.906\n",
      "Episode: 148, Reward: 105.0, avg loss: 0.03950, eps: 0.905\n",
      "Episode: 149, Reward: 340.0, avg loss: 0.04036, eps: 0.904\n",
      "Episode: 150, Reward: 230.0, avg loss: 0.03920, eps: 0.902\n",
      "Episode: 151, Reward: 215.0, avg loss: 0.03889, eps: 0.900\n",
      "Episode: 152, Reward: 155.0, avg loss: 0.04468, eps: 0.899\n",
      "Episode: 153, Reward: 210.0, avg loss: 0.04029, eps: 0.898\n",
      "Episode: 154, Reward: 410.0, avg loss: 0.04411, eps: 0.895\n",
      "Episode: 155, Reward: 145.0, avg loss: 0.04887, eps: 0.893\n",
      "Episode: 156, Reward: 65.0, avg loss: 0.05383, eps: 0.893\n",
      "Episode: 157, Reward: 155.0, avg loss: 0.04282, eps: 0.891\n",
      "Episode: 158, Reward: 30.0, avg loss: 0.04471, eps: 0.891\n",
      "Episode: 159, Reward: 135.0, avg loss: 0.04924, eps: 0.890\n",
      "Episode: 160, Reward: 225.0, avg loss: 0.04928, eps: 0.888\n",
      "Episode: 161, Reward: 15.0, avg loss: 0.04912, eps: 0.887\n",
      "Episode: 162, Reward: 110.0, avg loss: 0.04987, eps: 0.886\n",
      "Episode: 163, Reward: 90.0, avg loss: 0.03774, eps: 0.885\n",
      "Episode: 164, Reward: 240.0, avg loss: 0.04196, eps: 0.884\n",
      "Episode: 165, Reward: 160.0, avg loss: 0.03748, eps: 0.882\n",
      "Episode: 166, Reward: 210.0, avg loss: 0.04475, eps: 0.881\n",
      "Episode: 167, Reward: 65.0, avg loss: 0.03498, eps: 0.880\n",
      "Episode: 168, Reward: 300.0, avg loss: 0.04326, eps: 0.878\n",
      "Episode: 169, Reward: 95.0, avg loss: 0.03822, eps: 0.878\n",
      "Episode: 170, Reward: 135.0, avg loss: 0.03751, eps: 0.876\n",
      "Episode: 171, Reward: 225.0, avg loss: 0.04300, eps: 0.874\n",
      "Episode: 172, Reward: 155.0, avg loss: 0.04092, eps: 0.873\n",
      "Episode: 173, Reward: 55.0, avg loss: 0.04262, eps: 0.872\n",
      "Episode: 174, Reward: 155.0, avg loss: 0.05117, eps: 0.871\n",
      "Episode: 175, Reward: 150.0, avg loss: 0.04540, eps: 0.869\n",
      "Episode: 176, Reward: 375.0, avg loss: 0.04357, eps: 0.866\n",
      "Episode: 177, Reward: 20.0, avg loss: 0.04331, eps: 0.865\n",
      "Episode: 178, Reward: 45.0, avg loss: 0.04199, eps: 0.864\n",
      "Episode: 179, Reward: 50.0, avg loss: 0.03948, eps: 0.863\n",
      "Episode: 180, Reward: 525.0, avg loss: 0.04485, eps: 0.861\n",
      "Episode: 181, Reward: 425.0, avg loss: 0.04563, eps: 0.859\n",
      "Episode: 182, Reward: 60.0, avg loss: 0.04201, eps: 0.858\n",
      "Episode: 183, Reward: 75.0, avg loss: 0.04533, eps: 0.857\n",
      "Episode: 184, Reward: 180.0, avg loss: 0.05342, eps: 0.856\n",
      "Episode: 185, Reward: 345.0, avg loss: 0.04362, eps: 0.854\n",
      "Episode: 186, Reward: 140.0, avg loss: 0.04349, eps: 0.853\n",
      "Episode: 187, Reward: 120.0, avg loss: 0.03674, eps: 0.852\n",
      "Episode: 188, Reward: 135.0, avg loss: 0.03857, eps: 0.851\n",
      "Episode: 189, Reward: 110.0, avg loss: 0.03976, eps: 0.850\n",
      "Episode: 190, Reward: 95.0, avg loss: 0.05157, eps: 0.849\n",
      "Episode: 191, Reward: 135.0, avg loss: 0.04094, eps: 0.847\n",
      "Episode: 192, Reward: 60.0, avg loss: 0.04931, eps: 0.847\n",
      "Episode: 193, Reward: 120.0, avg loss: 0.04443, eps: 0.846\n",
      "Episode: 194, Reward: 380.0, avg loss: 0.04349, eps: 0.844\n",
      "Episode: 195, Reward: 35.0, avg loss: 0.04308, eps: 0.844\n",
      "Episode: 196, Reward: 225.0, avg loss: 0.04620, eps: 0.842\n",
      "Episode: 197, Reward: 185.0, avg loss: 0.04828, eps: 0.840\n",
      "Episode: 198, Reward: 110.0, avg loss: 0.05013, eps: 0.839\n",
      "Episode: 199, Reward: 155.0, avg loss: 0.04339, eps: 0.837\n",
      "Episode: 200, Reward: 50.0, avg loss: 0.04159, eps: 0.836\n",
      "Episode: 201, Reward: 105.0, avg loss: 0.04025, eps: 0.835\n",
      "Episode: 202, Reward: 110.0, avg loss: 0.04244, eps: 0.834\n",
      "Episode: 203, Reward: 20.0, avg loss: 0.03978, eps: 0.833\n",
      "Episode: 204, Reward: 75.0, avg loss: 0.03797, eps: 0.832\n",
      "Episode: 205, Reward: 80.0, avg loss: 0.04066, eps: 0.831\n",
      "Episode: 206, Reward: 110.0, avg loss: 0.04011, eps: 0.830\n",
      "Episode: 207, Reward: 10.0, avg loss: 0.05018, eps: 0.829\n",
      "Episode: 208, Reward: 210.0, avg loss: 0.04603, eps: 0.828\n",
      "Episode: 209, Reward: 140.0, avg loss: 0.04298, eps: 0.827\n",
      "Episode: 210, Reward: 80.0, avg loss: 0.03452, eps: 0.826\n",
      "Episode: 211, Reward: 60.0, avg loss: 0.04389, eps: 0.825\n",
      "Episode: 212, Reward: 80.0, avg loss: 0.05430, eps: 0.824\n",
      "Episode: 213, Reward: 245.0, avg loss: 0.04895, eps: 0.822\n",
      "Episode: 214, Reward: 110.0, avg loss: 0.04764, eps: 0.820\n",
      "Episode: 215, Reward: 175.0, avg loss: 0.04624, eps: 0.819\n",
      "Episode: 216, Reward: 35.0, avg loss: 0.04515, eps: 0.818\n",
      "Episode: 217, Reward: 80.0, avg loss: 0.04467, eps: 0.817\n",
      "Episode: 218, Reward: 80.0, avg loss: 0.04499, eps: 0.816\n",
      "Episode: 219, Reward: 90.0, avg loss: 0.04064, eps: 0.816\n",
      "Episode: 220, Reward: 155.0, avg loss: 0.04291, eps: 0.814\n",
      "Episode: 221, Reward: 20.0, avg loss: 0.04582, eps: 0.813\n",
      "Episode: 222, Reward: 85.0, avg loss: 0.05207, eps: 0.812\n",
      "Episode: 223, Reward: 115.0, avg loss: 0.04285, eps: 0.810\n",
      "Episode: 224, Reward: 120.0, avg loss: 0.04503, eps: 0.809\n",
      "Episode: 225, Reward: 70.0, avg loss: 0.04854, eps: 0.808\n",
      "Episode: 226, Reward: 190.0, avg loss: 0.04425, eps: 0.807\n",
      "Episode: 227, Reward: 65.0, avg loss: 0.04379, eps: 0.806\n",
      "Episode: 228, Reward: 35.0, avg loss: 0.04780, eps: 0.805\n",
      "Episode: 229, Reward: 195.0, avg loss: 0.03890, eps: 0.803\n",
      "Episode: 230, Reward: 180.0, avg loss: 0.04436, eps: 0.802\n",
      "Episode: 231, Reward: 95.0, avg loss: 0.04700, eps: 0.801\n",
      "Episode: 232, Reward: 90.0, avg loss: 0.04315, eps: 0.800\n",
      "Episode: 233, Reward: 45.0, avg loss: 0.04477, eps: 0.799\n",
      "Episode: 234, Reward: 20.0, avg loss: 0.04718, eps: 0.798\n",
      "Episode: 235, Reward: 95.0, avg loss: 0.05459, eps: 0.797\n",
      "Episode: 236, Reward: 120.0, avg loss: 0.05949, eps: 0.796\n",
      "Episode: 237, Reward: 405.0, avg loss: 0.04486, eps: 0.794\n",
      "Episode: 238, Reward: 60.0, avg loss: 0.04696, eps: 0.794\n",
      "Episode: 239, Reward: 110.0, avg loss: 0.04960, eps: 0.793\n",
      "Episode: 240, Reward: 20.0, avg loss: 0.03730, eps: 0.792\n",
      "Episode: 241, Reward: 185.0, avg loss: 0.03229, eps: 0.791\n",
      "Episode: 242, Reward: 365.0, avg loss: 0.03414, eps: 0.789\n",
      "Episode: 243, Reward: 155.0, avg loss: 0.03344, eps: 0.788\n",
      "Episode: 244, Reward: 100.0, avg loss: 0.03034, eps: 0.787\n",
      "Episode: 245, Reward: 140.0, avg loss: 0.03341, eps: 0.786\n",
      "Episode: 246, Reward: 380.0, avg loss: 0.02795, eps: 0.784\n",
      "Episode: 247, Reward: 185.0, avg loss: 0.03790, eps: 0.783\n",
      "Episode: 248, Reward: 85.0, avg loss: 0.03262, eps: 0.782\n",
      "Episode: 249, Reward: 340.0, avg loss: 0.03386, eps: 0.780\n",
      "Episode: 250, Reward: 145.0, avg loss: 0.03360, eps: 0.779\n",
      "Episode: 251, Reward: 155.0, avg loss: 0.03494, eps: 0.778\n",
      "Episode: 252, Reward: 380.0, avg loss: 0.03399, eps: 0.775\n",
      "Episode: 253, Reward: 160.0, avg loss: 0.04076, eps: 0.774\n",
      "Episode: 254, Reward: 35.0, avg loss: 0.03893, eps: 0.773\n",
      "Episode: 255, Reward: 90.0, avg loss: 0.04115, eps: 0.772\n",
      "Episode: 256, Reward: 170.0, avg loss: 0.03665, eps: 0.771\n",
      "Episode: 257, Reward: 45.0, avg loss: 0.03730, eps: 0.769\n",
      "Episode: 258, Reward: 210.0, avg loss: 0.03659, eps: 0.768\n",
      "Episode: 259, Reward: 110.0, avg loss: 0.03730, eps: 0.767\n",
      "Episode: 260, Reward: 370.0, avg loss: 0.03139, eps: 0.765\n",
      "Episode: 261, Reward: 90.0, avg loss: 0.03658, eps: 0.764\n",
      "Episode: 262, Reward: 55.0, avg loss: 0.03326, eps: 0.763\n",
      "Episode: 263, Reward: 50.0, avg loss: 0.03454, eps: 0.762\n",
      "Episode: 264, Reward: 90.0, avg loss: 0.02557, eps: 0.761\n",
      "Episode: 265, Reward: 80.0, avg loss: 0.02893, eps: 0.760\n",
      "Episode: 266, Reward: 215.0, avg loss: 0.02874, eps: 0.758\n",
      "Episode: 267, Reward: 60.0, avg loss: 0.03296, eps: 0.756\n",
      "Episode: 268, Reward: 65.0, avg loss: 0.03072, eps: 0.755\n",
      "Episode: 269, Reward: 210.0, avg loss: 0.03337, eps: 0.754\n",
      "Episode: 270, Reward: 80.0, avg loss: 0.03375, eps: 0.753\n",
      "Episode: 271, Reward: 120.0, avg loss: 0.03441, eps: 0.752\n",
      "Episode: 272, Reward: 135.0, avg loss: 0.03521, eps: 0.750\n",
      "Episode: 273, Reward: 245.0, avg loss: 0.03695, eps: 0.749\n",
      "Episode: 274, Reward: 135.0, avg loss: 0.03349, eps: 0.748\n",
      "Episode: 275, Reward: 50.0, avg loss: 0.02934, eps: 0.747\n",
      "Episode: 276, Reward: 50.0, avg loss: 0.03262, eps: 0.746\n",
      "Episode: 277, Reward: 85.0, avg loss: 0.03246, eps: 0.745\n",
      "Episode: 278, Reward: 155.0, avg loss: 0.03641, eps: 0.744\n",
      "Episode: 279, Reward: 50.0, avg loss: 0.03744, eps: 0.743\n",
      "Episode: 280, Reward: 130.0, avg loss: 0.03325, eps: 0.742\n",
      "Episode: 281, Reward: 60.0, avg loss: 0.03433, eps: 0.741\n",
      "Episode: 282, Reward: 155.0, avg loss: 0.03653, eps: 0.740\n",
      "Episode: 283, Reward: 120.0, avg loss: 0.03234, eps: 0.739\n",
      "Episode: 284, Reward: 125.0, avg loss: 0.03309, eps: 0.738\n",
      "Episode: 285, Reward: 40.0, avg loss: 0.03372, eps: 0.737\n",
      "Episode: 286, Reward: 185.0, avg loss: 0.03713, eps: 0.735\n",
      "Episode: 287, Reward: 135.0, avg loss: 0.03132, eps: 0.734\n",
      "Episode: 288, Reward: 50.0, avg loss: 0.03158, eps: 0.733\n",
      "Episode: 289, Reward: 165.0, avg loss: 0.03431, eps: 0.731\n",
      "Episode: 290, Reward: 180.0, avg loss: 0.03309, eps: 0.730\n",
      "Episode: 291, Reward: 85.0, avg loss: 0.03802, eps: 0.729\n",
      "Episode: 292, Reward: 110.0, avg loss: 0.03637, eps: 0.728\n",
      "Episode: 293, Reward: 45.0, avg loss: 0.03423, eps: 0.728\n",
      "Episode: 294, Reward: 85.0, avg loss: 0.03375, eps: 0.726\n",
      "Episode: 295, Reward: 105.0, avg loss: 0.03226, eps: 0.725\n",
      "Episode: 296, Reward: 365.0, avg loss: 0.03539, eps: 0.724\n",
      "Episode: 297, Reward: 120.0, avg loss: 0.03234, eps: 0.722\n",
      "Episode: 298, Reward: 250.0, avg loss: 0.03213, eps: 0.721\n",
      "Episode: 299, Reward: 165.0, avg loss: 0.03660, eps: 0.719\n",
      "Episode: 300, Reward: 210.0, avg loss: 0.03150, eps: 0.718\n",
      "Episode: 301, Reward: 405.0, avg loss: 0.03379, eps: 0.716\n",
      "Episode: 302, Reward: 415.0, avg loss: 0.03612, eps: 0.714\n",
      "Episode: 303, Reward: 115.0, avg loss: 0.03330, eps: 0.713\n",
      "Episode: 304, Reward: 185.0, avg loss: 0.03080, eps: 0.712\n",
      "Episode: 305, Reward: 355.0, avg loss: 0.02788, eps: 0.711\n",
      "Episode: 306, Reward: 185.0, avg loss: 0.03658, eps: 0.709\n",
      "Episode: 307, Reward: 140.0, avg loss: 0.03116, eps: 0.708\n",
      "Episode: 308, Reward: 155.0, avg loss: 0.03287, eps: 0.706\n",
      "Episode: 309, Reward: 80.0, avg loss: 0.03200, eps: 0.705\n",
      "Episode: 310, Reward: 185.0, avg loss: 0.03515, eps: 0.704\n",
      "Episode: 311, Reward: 180.0, avg loss: 0.03089, eps: 0.703\n",
      "Episode: 312, Reward: 210.0, avg loss: 0.03273, eps: 0.701\n",
      "Episode: 313, Reward: 200.0, avg loss: 0.02989, eps: 0.699\n",
      "Episode: 314, Reward: 410.0, avg loss: 0.02936, eps: 0.698\n",
      "Episode: 315, Reward: 95.0, avg loss: 0.03820, eps: 0.697\n",
      "Episode: 316, Reward: 105.0, avg loss: 0.03688, eps: 0.696\n",
      "Episode: 317, Reward: 135.0, avg loss: 0.03209, eps: 0.694\n",
      "Episode: 318, Reward: 165.0, avg loss: 0.03280, eps: 0.693\n",
      "Episode: 319, Reward: 500.0, avg loss: 0.03248, eps: 0.691\n",
      "Episode: 320, Reward: 160.0, avg loss: 0.03277, eps: 0.689\n",
      "Episode: 321, Reward: 390.0, avg loss: 0.03527, eps: 0.688\n",
      "Episode: 322, Reward: 70.0, avg loss: 0.03633, eps: 0.687\n",
      "Episode: 323, Reward: 585.0, avg loss: 0.03565, eps: 0.685\n",
      "Episode: 324, Reward: 235.0, avg loss: 0.03933, eps: 0.683\n",
      "Episode: 325, Reward: 55.0, avg loss: 0.03360, eps: 0.682\n",
      "Episode: 326, Reward: 180.0, avg loss: 0.04026, eps: 0.681\n",
      "Episode: 327, Reward: 75.0, avg loss: 0.03691, eps: 0.680\n",
      "Episode: 328, Reward: 160.0, avg loss: 0.03394, eps: 0.679\n",
      "Episode: 329, Reward: 20.0, avg loss: 0.03412, eps: 0.678\n",
      "Episode: 330, Reward: 195.0, avg loss: 0.03513, eps: 0.677\n",
      "Episode: 331, Reward: 210.0, avg loss: 0.03257, eps: 0.675\n",
      "Episode: 332, Reward: 235.0, avg loss: 0.03456, eps: 0.673\n",
      "Episode: 333, Reward: 125.0, avg loss: 0.03581, eps: 0.672\n",
      "Episode: 334, Reward: 290.0, avg loss: 0.03460, eps: 0.670\n",
      "Episode: 335, Reward: 75.0, avg loss: 0.03157, eps: 0.669\n",
      "Episode: 336, Reward: 195.0, avg loss: 0.03607, eps: 0.668\n",
      "Episode: 337, Reward: 215.0, avg loss: 0.03272, eps: 0.666\n",
      "Episode: 338, Reward: 680.0, avg loss: 0.03368, eps: 0.664\n",
      "Episode: 339, Reward: 80.0, avg loss: 0.03541, eps: 0.663\n",
      "Episode: 340, Reward: 175.0, avg loss: 0.03551, eps: 0.661\n",
      "Episode: 341, Reward: 75.0, avg loss: 0.03754, eps: 0.660\n",
      "Episode: 342, Reward: 180.0, avg loss: 0.04349, eps: 0.659\n",
      "Episode: 343, Reward: 110.0, avg loss: 0.03229, eps: 0.658\n",
      "Episode: 344, Reward: 165.0, avg loss: 0.03316, eps: 0.656\n",
      "Episode: 345, Reward: 155.0, avg loss: 0.04270, eps: 0.655\n",
      "Episode: 346, Reward: 210.0, avg loss: 0.03944, eps: 0.653\n",
      "Episode: 347, Reward: 255.0, avg loss: 0.04097, eps: 0.651\n",
      "Episode: 348, Reward: 55.0, avg loss: 0.03304, eps: 0.650\n",
      "Episode: 349, Reward: 165.0, avg loss: 0.03126, eps: 0.649\n",
      "Episode: 350, Reward: 95.0, avg loss: 0.03362, eps: 0.647\n",
      "Episode: 351, Reward: 385.0, avg loss: 0.03631, eps: 0.645\n",
      "Episode: 352, Reward: 115.0, avg loss: 0.03450, eps: 0.644\n",
      "Episode: 353, Reward: 190.0, avg loss: 0.03090, eps: 0.643\n",
      "Episode: 354, Reward: 175.0, avg loss: 0.03998, eps: 0.641\n",
      "Episode: 355, Reward: 210.0, avg loss: 0.03303, eps: 0.640\n",
      "Episode: 356, Reward: 290.0, avg loss: 0.04066, eps: 0.638\n",
      "Episode: 357, Reward: 105.0, avg loss: 0.03436, eps: 0.637\n",
      "Episode: 358, Reward: 100.0, avg loss: 0.03897, eps: 0.636\n",
      "Episode: 359, Reward: 120.0, avg loss: 0.03654, eps: 0.635\n",
      "Episode: 360, Reward: 170.0, avg loss: 0.03131, eps: 0.634\n",
      "Episode: 361, Reward: 615.0, avg loss: 0.04010, eps: 0.631\n",
      "Episode: 362, Reward: 120.0, avg loss: 0.03479, eps: 0.629\n",
      "Episode: 363, Reward: 50.0, avg loss: 0.03583, eps: 0.629\n",
      "Episode: 364, Reward: 140.0, avg loss: 0.03115, eps: 0.627\n",
      "Episode: 365, Reward: 220.0, avg loss: 0.03975, eps: 0.626\n",
      "Episode: 366, Reward: 90.0, avg loss: 0.03798, eps: 0.625\n",
      "Episode: 367, Reward: 110.0, avg loss: 0.03670, eps: 0.624\n",
      "Episode: 368, Reward: 10.0, avg loss: 0.03583, eps: 0.623\n",
      "Episode: 369, Reward: 330.0, avg loss: 0.03597, eps: 0.622\n",
      "Episode: 370, Reward: 65.0, avg loss: 0.03981, eps: 0.620\n",
      "Episode: 371, Reward: 115.0, avg loss: 0.03873, eps: 0.619\n",
      "Episode: 372, Reward: 435.0, avg loss: 0.03960, eps: 0.618\n",
      "Episode: 373, Reward: 250.0, avg loss: 0.03489, eps: 0.616\n",
      "Episode: 374, Reward: 50.0, avg loss: 0.02552, eps: 0.615\n",
      "Episode: 375, Reward: 140.0, avg loss: 0.03808, eps: 0.614\n",
      "Episode: 376, Reward: 450.0, avg loss: 0.03422, eps: 0.612\n",
      "Episode: 377, Reward: 510.0, avg loss: 0.04157, eps: 0.610\n",
      "Episode: 378, Reward: 100.0, avg loss: 0.03312, eps: 0.609\n",
      "Episode: 379, Reward: 80.0, avg loss: 0.04140, eps: 0.608\n",
      "Episode: 380, Reward: 80.0, avg loss: 0.04157, eps: 0.607\n",
      "Episode: 381, Reward: 165.0, avg loss: 0.03650, eps: 0.606\n",
      "Episode: 382, Reward: 110.0, avg loss: 0.03864, eps: 0.605\n",
      "Episode: 383, Reward: 135.0, avg loss: 0.03860, eps: 0.603\n",
      "Episode: 384, Reward: 125.0, avg loss: 0.03500, eps: 0.602\n",
      "Episode: 385, Reward: 150.0, avg loss: 0.04043, eps: 0.601\n",
      "Episode: 386, Reward: 45.0, avg loss: 0.04441, eps: 0.600\n",
      "Episode: 387, Reward: 210.0, avg loss: 0.03207, eps: 0.598\n",
      "Episode: 388, Reward: 35.0, avg loss: 0.03813, eps: 0.598\n",
      "Episode: 389, Reward: 75.0, avg loss: 0.03469, eps: 0.597\n",
      "Episode: 390, Reward: 410.0, avg loss: 0.03690, eps: 0.595\n",
      "Episode: 391, Reward: 110.0, avg loss: 0.03617, eps: 0.594\n",
      "Episode: 392, Reward: 165.0, avg loss: 0.03747, eps: 0.593\n",
      "Episode: 393, Reward: 385.0, avg loss: 0.03329, eps: 0.590\n",
      "Episode: 394, Reward: 230.0, avg loss: 0.03770, eps: 0.589\n",
      "Episode: 395, Reward: 215.0, avg loss: 0.03990, eps: 0.587\n",
      "Episode: 396, Reward: 45.0, avg loss: 0.02998, eps: 0.586\n",
      "Episode: 397, Reward: 220.0, avg loss: 0.03738, eps: 0.584\n",
      "Episode: 398, Reward: 165.0, avg loss: 0.03882, eps: 0.582\n",
      "Episode: 399, Reward: 455.0, avg loss: 0.04021, eps: 0.580\n",
      "Episode: 400, Reward: 285.0, avg loss: 0.03683, eps: 0.578\n",
      "Episode: 401, Reward: 65.0, avg loss: 0.03565, eps: 0.577\n",
      "Episode: 402, Reward: 110.0, avg loss: 0.04082, eps: 0.576\n",
      "Episode: 403, Reward: 20.0, avg loss: 0.03490, eps: 0.575\n",
      "Episode: 404, Reward: 155.0, avg loss: 0.04741, eps: 0.574\n",
      "Episode: 405, Reward: 130.0, avg loss: 0.03587, eps: 0.573\n",
      "Episode: 406, Reward: 100.0, avg loss: 0.03435, eps: 0.572\n",
      "Episode: 407, Reward: 505.0, avg loss: 0.03603, eps: 0.570\n",
      "Episode: 408, Reward: 215.0, avg loss: 0.03526, eps: 0.568\n",
      "Episode: 409, Reward: 145.0, avg loss: 0.03647, eps: 0.566\n",
      "Episode: 410, Reward: 135.0, avg loss: 0.03330, eps: 0.565\n",
      "Episode: 411, Reward: 215.0, avg loss: 0.03660, eps: 0.564\n",
      "Episode: 412, Reward: 20.0, avg loss: 0.03279, eps: 0.563\n",
      "Episode: 413, Reward: 65.0, avg loss: 0.03347, eps: 0.562\n",
      "Episode: 414, Reward: 105.0, avg loss: 0.03504, eps: 0.561\n",
      "Episode: 415, Reward: 135.0, avg loss: 0.04138, eps: 0.559\n",
      "Episode: 416, Reward: 65.0, avg loss: 0.03982, eps: 0.558\n",
      "Episode: 417, Reward: 180.0, avg loss: 0.03473, eps: 0.557\n",
      "Episode: 418, Reward: 195.0, avg loss: 0.03822, eps: 0.555\n",
      "Episode: 419, Reward: 130.0, avg loss: 0.04184, eps: 0.554\n",
      "Episode: 420, Reward: 315.0, avg loss: 0.03566, eps: 0.552\n",
      "Episode: 421, Reward: 90.0, avg loss: 0.03390, eps: 0.551\n",
      "Episode: 422, Reward: 60.0, avg loss: 0.03546, eps: 0.550\n",
      "Episode: 423, Reward: 120.0, avg loss: 0.04275, eps: 0.548\n",
      "Episode: 424, Reward: 390.0, avg loss: 0.03634, eps: 0.547\n",
      "Episode: 425, Reward: 200.0, avg loss: 0.03530, eps: 0.545\n",
      "Episode: 426, Reward: 210.0, avg loss: 0.03719, eps: 0.544\n",
      "Episode: 427, Reward: 45.0, avg loss: 0.04124, eps: 0.543\n",
      "Episode: 428, Reward: 150.0, avg loss: 0.03468, eps: 0.542\n",
      "Episode: 429, Reward: 255.0, avg loss: 0.03619, eps: 0.540\n",
      "Episode: 430, Reward: 535.0, avg loss: 0.03699, eps: 0.538\n",
      "Episode: 431, Reward: 185.0, avg loss: 0.03163, eps: 0.537\n",
      "Episode: 432, Reward: 75.0, avg loss: 0.03923, eps: 0.536\n",
      "Episode: 433, Reward: 285.0, avg loss: 0.03745, eps: 0.534\n",
      "Episode: 434, Reward: 70.0, avg loss: 0.03603, eps: 0.533\n",
      "Episode: 435, Reward: 80.0, avg loss: 0.03559, eps: 0.532\n",
      "Episode: 436, Reward: 240.0, avg loss: 0.03576, eps: 0.530\n",
      "Episode: 437, Reward: 215.0, avg loss: 0.03612, eps: 0.528\n",
      "Episode: 438, Reward: 65.0, avg loss: 0.04442, eps: 0.527\n",
      "Episode: 439, Reward: 10.0, avg loss: 0.03657, eps: 0.526\n",
      "Episode: 440, Reward: 210.0, avg loss: 0.03467, eps: 0.525\n",
      "Episode: 441, Reward: 55.0, avg loss: 0.03325, eps: 0.524\n",
      "Episode: 442, Reward: 75.0, avg loss: 0.03144, eps: 0.523\n",
      "Episode: 443, Reward: 150.0, avg loss: 0.03085, eps: 0.521\n",
      "Episode: 444, Reward: 85.0, avg loss: 0.04183, eps: 0.520\n",
      "Episode: 445, Reward: 355.0, avg loss: 0.03617, eps: 0.518\n",
      "Episode: 446, Reward: 90.0, avg loss: 0.03411, eps: 0.518\n",
      "Episode: 447, Reward: 355.0, avg loss: 0.03853, eps: 0.516\n",
      "Episode: 448, Reward: 280.0, avg loss: 0.04186, eps: 0.514\n",
      "Episode: 449, Reward: 590.0, avg loss: 0.03879, eps: 0.512\n",
      "Episode: 450, Reward: 360.0, avg loss: 0.04136, eps: 0.511\n",
      "Episode: 451, Reward: 130.0, avg loss: 0.03639, eps: 0.509\n",
      "Episode: 452, Reward: 105.0, avg loss: 0.03292, eps: 0.509\n",
      "Episode: 453, Reward: 125.0, avg loss: 0.03754, eps: 0.508\n",
      "Episode: 454, Reward: 155.0, avg loss: 0.03758, eps: 0.506\n",
      "Episode: 455, Reward: 120.0, avg loss: 0.03478, eps: 0.505\n",
      "Episode: 456, Reward: 385.0, avg loss: 0.03638, eps: 0.503\n",
      "Episode: 457, Reward: 60.0, avg loss: 0.03236, eps: 0.502\n",
      "Episode: 458, Reward: 130.0, avg loss: 0.04091, eps: 0.501\n",
      "Episode: 459, Reward: 190.0, avg loss: 0.03684, eps: 0.500\n",
      "Episode: 460, Reward: 125.0, avg loss: 0.03301, eps: 0.498\n",
      "Episode: 461, Reward: 120.0, avg loss: 0.03843, eps: 0.497\n",
      "Episode: 462, Reward: 75.0, avg loss: 0.03584, eps: 0.496\n",
      "Episode: 463, Reward: 295.0, avg loss: 0.03775, eps: 0.495\n",
      "Episode: 464, Reward: 100.0, avg loss: 0.03611, eps: 0.494\n",
      "Episode: 465, Reward: 50.0, avg loss: 0.03870, eps: 0.492\n",
      "Episode: 466, Reward: 365.0, avg loss: 0.03444, eps: 0.491\n",
      "Episode: 467, Reward: 105.0, avg loss: 0.03329, eps: 0.490\n",
      "Episode: 468, Reward: 270.0, avg loss: 0.03778, eps: 0.488\n",
      "Episode: 469, Reward: 110.0, avg loss: 0.03639, eps: 0.487\n",
      "Episode: 470, Reward: 340.0, avg loss: 0.04068, eps: 0.485\n",
      "Episode: 471, Reward: 195.0, avg loss: 0.03678, eps: 0.483\n",
      "Episode: 472, Reward: 125.0, avg loss: 0.04049, eps: 0.482\n",
      "Episode: 473, Reward: 135.0, avg loss: 0.04155, eps: 0.481\n",
      "Episode: 474, Reward: 175.0, avg loss: 0.03933, eps: 0.479\n",
      "Episode: 475, Reward: 185.0, avg loss: 0.03745, eps: 0.478\n",
      "Episode: 476, Reward: 175.0, avg loss: 0.03606, eps: 0.477\n",
      "Episode: 477, Reward: 105.0, avg loss: 0.04149, eps: 0.475\n",
      "Episode: 478, Reward: 600.0, avg loss: 0.03572, eps: 0.471\n",
      "Episode: 479, Reward: 210.0, avg loss: 0.04273, eps: 0.470\n",
      "Episode: 480, Reward: 270.0, avg loss: 0.03646, eps: 0.468\n",
      "Episode: 481, Reward: 140.0, avg loss: 0.03546, eps: 0.466\n",
      "Episode: 482, Reward: 25.0, avg loss: 0.03640, eps: 0.465\n",
      "Episode: 483, Reward: 80.0, avg loss: 0.03806, eps: 0.465\n",
      "Episode: 484, Reward: 175.0, avg loss: 0.03907, eps: 0.463\n",
      "Episode: 485, Reward: 365.0, avg loss: 0.03818, eps: 0.461\n",
      "Episode: 486, Reward: 60.0, avg loss: 0.03740, eps: 0.460\n",
      "Episode: 487, Reward: 420.0, avg loss: 0.03829, eps: 0.458\n",
      "Episode: 488, Reward: 150.0, avg loss: 0.03914, eps: 0.457\n",
      "Episode: 489, Reward: 125.0, avg loss: 0.04668, eps: 0.456\n",
      "Episode: 490, Reward: 220.0, avg loss: 0.03878, eps: 0.455\n",
      "Episode: 491, Reward: 215.0, avg loss: 0.03552, eps: 0.454\n",
      "Episode: 492, Reward: 110.0, avg loss: 0.04120, eps: 0.453\n",
      "Episode: 493, Reward: 35.0, avg loss: 0.03711, eps: 0.452\n",
      "Episode: 494, Reward: 195.0, avg loss: 0.03459, eps: 0.450\n",
      "Episode: 495, Reward: 110.0, avg loss: 0.03704, eps: 0.449\n",
      "Episode: 496, Reward: 245.0, avg loss: 0.03686, eps: 0.447\n",
      "Episode: 497, Reward: 355.0, avg loss: 0.03938, eps: 0.445\n",
      "Episode: 498, Reward: 225.0, avg loss: 0.03991, eps: 0.443\n",
      "Episode: 499, Reward: 120.0, avg loss: 0.03744, eps: 0.442\n",
      "Episode: 500, Reward: 170.0, avg loss: 0.03620, eps: 0.441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d8d3a04fdf6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mDELAY_TRAINING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdouble_q\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mupdate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d8d3a04fdf6a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(primary_network, memory, target_network)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# predict Q(s,a) given the batch of states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprim_qt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d8d3a04fdf6a>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m                              dtype=np.float32)\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNUM_FRAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mNUM_FRAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import imageio\n",
    "\n",
    "STORE_PATH = 'TensorBoard'\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_MIN_ITER = 500000\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "TAU = 0.08\n",
    "POST_PROCESS_IMAGE_SIZE = (105, 80, 1)\n",
    "DELAY_TRAINING = 50000\n",
    "NUM_FRAMES = 4\n",
    "GIF_RECORDING_FREQ = 100\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "class DQModel(keras.Model):\n",
    "    def __init__(self, hidden_size: int, num_actions: int, dueling: bool):\n",
    "        super(DQModel, self).__init__()\n",
    "        self.dueling = dueling\n",
    "        self.conv1 = keras.layers.Conv2D(16, (8, 8), (4, 4), activation='relu')\n",
    "        self.conv2 = keras.layers.Conv2D(32, (4, 4), (2, 2), activation='relu')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.adv_dense = keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_out = keras.layers.Dense(num_actions,\n",
    "                                          kernel_initializer=keras.initializers.he_normal())\n",
    "        if dueling:\n",
    "            self.v_dense = keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_out = keras.layers.Dense(1, kernel_initializer=keras.initializers.he_normal())\n",
    "            self.lambda_layer = keras.layers.Lambda(lambda x: x - tf.reduce_mean(x))\n",
    "            self.combine = keras.layers.Add()\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        adv = self.adv_dense(x)\n",
    "        adv = self.adv_out(adv)\n",
    "        if self.dueling:\n",
    "            v = self.v_dense(x)\n",
    "            v = self.v_out(v)\n",
    "            norm_adv = self.lambda_layer(adv)\n",
    "            combined = self.combine([v, norm_adv])\n",
    "            return combined\n",
    "        return adv\n",
    "\n",
    "primary_network = DQModel(256, num_actions, True)\n",
    "target_network = DQModel(256, num_actions, True)\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "# make target_network = primary_network\n",
    "for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "    t.assign(e)\n",
    "\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._actions = np.zeros(max_memory, dtype=np.int32)\n",
    "        self._rewards = np.zeros(max_memory, dtype=np.float32)\n",
    "        self._frames = np.zeros((POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], max_memory), dtype=np.float32)\n",
    "        self._terminal = np.zeros(max_memory, dtype=np.bool)\n",
    "        self._i = 0\n",
    "\n",
    "    def add_sample(self, frame, action, reward, terminal):\n",
    "        self._actions[self._i] = action\n",
    "        self._rewards[self._i] = reward\n",
    "        self._frames[:, :, self._i] = frame[:, :, 0]\n",
    "        self._terminal[self._i] = terminal\n",
    "        if self._i % (self._max_memory - 1) == 0 and self._i != 0:\n",
    "            self._i = BATCH_SIZE + NUM_FRAMES + 1\n",
    "        else:\n",
    "            self._i += 1\n",
    "\n",
    "    def sample(self):\n",
    "        if self._i < BATCH_SIZE + NUM_FRAMES + 1:\n",
    "            raise ValueError(\"Not enough memory to extract a batch\")\n",
    "        else:\n",
    "            rand_idxs = np.random.randint(NUM_FRAMES + 1, self._i, size=BATCH_SIZE)\n",
    "            states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            next_states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            for i, idx in enumerate(rand_idxs):\n",
    "                states[i] = self._frames[:, :, idx - 1 - NUM_FRAMES:idx - 1]\n",
    "                next_states[i] = self._frames[:, :, idx - NUM_FRAMES:idx]\n",
    "            return states, self._actions[rand_idxs], self._rewards[rand_idxs], next_states, self._terminal[rand_idxs]\n",
    "\n",
    "\n",
    "memory = Memory(500000)\n",
    "# memory = Memory(100)\n",
    "\n",
    "\n",
    "def image_preprocess(image, new_size=(105, 80)):\n",
    "    # convert to greyscale, resize and normalize the image\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image, new_size)\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def choose_action(state, primary_network, eps, step):\n",
    "    if step < DELAY_TRAINING:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(primary_network(tf.reshape(state, (1, POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                           POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES)).numpy()))\n",
    "\n",
    "\n",
    "def update_network(primary_network, target_network):\n",
    "    # update target network parameters slowly from primary network\n",
    "    for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "        t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "\n",
    "def process_state_stack(state_stack, state):\n",
    "    for i in range(1, state_stack.shape[-1]):\n",
    "        state_stack[:, :, i - 1].assign(state_stack[:, :, i])\n",
    "    state_stack[:, :, -1].assign(state[:, :, 0])\n",
    "    return state_stack\n",
    "\n",
    "\n",
    "def record_gif(frame_list, episode, fps=50):\n",
    "    imageio.mimsave(STORE_PATH + f\"/SPACE_INVADERS_EPISODE-{episode}.gif\", frame_list, fps=fps) #duration=duration_per_frame)\n",
    "\n",
    "\n",
    "def train(primary_network, memory, target_network=None):\n",
    "    states, actions, rewards, next_states, terminal = memory.sample()\n",
    "    # predict Q(s,a) given the batch of states\n",
    "    prim_qt = primary_network(states)\n",
    "    # predict Q(s',a') from the evaluation network\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "    target_q = prim_qt.numpy()\n",
    "    updates = rewards\n",
    "    valid_idxs = terminal != True\n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    if target_network is None:\n",
    "        updates[valid_idxs] += GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1)\n",
    "    else:\n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "        q_from_target = target_network(next_states)\n",
    "        updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "    target_q[batch_idxs, actions] = updates\n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss\n",
    "\n",
    "num_episodes = 1000000\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/DuelingQSI_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "double_q = True\n",
    "steps = 0\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = image_preprocess(state)\n",
    "    state_stack = tf.Variable(np.repeat(state.numpy(), NUM_FRAMES).reshape((POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                                            POST_PROCESS_IMAGE_SIZE[1],\n",
    "                                                                            NUM_FRAMES)))\n",
    "    cnt = 1\n",
    "    avg_loss = 0\n",
    "    tot_reward = 0\n",
    "    if i % GIF_RECORDING_FREQ == 0:\n",
    "        frame_list = []\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = choose_action(state_stack, primary_network, eps, steps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        tot_reward += reward\n",
    "        if i % GIF_RECORDING_FREQ == 0:\n",
    "            frame_list.append(tf.cast(tf.image.resize(next_state, (480, 320)), tf.uint8).numpy())\n",
    "        next_state = image_preprocess(next_state)\n",
    "        state_stack = process_state_stack(state_stack, next_state)\n",
    "        # store in memory\n",
    "        memory.add_sample(next_state, action, reward, done)\n",
    "\n",
    "        if steps > DELAY_TRAINING:\n",
    "            loss = train(primary_network, memory, target_network if double_q else None)\n",
    "            update_network(primary_network, target_network)\n",
    "        else:\n",
    "            loss = -1\n",
    "        avg_loss += loss\n",
    "\n",
    "        # linearly decay the eps value\n",
    "        if steps > DELAY_TRAINING:\n",
    "            eps = MAX_EPSILON - ((steps - DELAY_TRAINING) / EPSILON_MIN_ITER) * \\\n",
    "                  (MAX_EPSILON - MIN_EPSILON) if steps < EPSILON_MIN_ITER else \\\n",
    "                MIN_EPSILON\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            if steps > DELAY_TRAINING:\n",
    "                avg_loss /= cnt\n",
    "                print(f\"Episode: {i}, Reward: {tot_reward}, avg loss: {avg_loss:.5f}, eps: {eps:.3f}\")\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.scalar('reward', tot_reward, step=i)\n",
    "                    tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            else:\n",
    "                print(f\"Pre-training...Episode: {i}\\r\", end=\"\")\n",
    "            if i % GIF_RECORDING_FREQ == 0:\n",
    "                record_gif(frame_list, i)\n",
    "            break\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
