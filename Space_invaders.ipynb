{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 73, Reward: 40.0, avg loss: -0.14784, eps: 0.999\n",
      "Episode: 74, Reward: 90.0, avg loss: 0.03072, eps: 0.998\n",
      "Episode: 75, Reward: 210.0, avg loss: 0.03526, eps: 0.997\n",
      "Episode: 76, Reward: 180.0, avg loss: 0.03008, eps: 0.996\n",
      "Episode: 77, Reward: 65.0, avg loss: 0.02978, eps: 0.995\n",
      "Episode: 78, Reward: 315.0, avg loss: 0.03364, eps: 0.993\n",
      "Episode: 79, Reward: 105.0, avg loss: 0.03575, eps: 0.992\n",
      "Episode: 80, Reward: 180.0, avg loss: 0.03280, eps: 0.991\n",
      "Episode: 81, Reward: 110.0, avg loss: 0.03193, eps: 0.990\n",
      "Episode: 82, Reward: 215.0, avg loss: 0.03367, eps: 0.988\n",
      "Episode: 83, Reward: 20.0, avg loss: 0.04044, eps: 0.987\n",
      "Episode: 84, Reward: 120.0, avg loss: 0.03422, eps: 0.986\n",
      "Episode: 85, Reward: 270.0, avg loss: 0.03332, eps: 0.985\n",
      "Episode: 86, Reward: 110.0, avg loss: 0.03255, eps: 0.984\n",
      "Episode: 87, Reward: 35.0, avg loss: 0.02913, eps: 0.983\n",
      "Episode: 88, Reward: 105.0, avg loss: 0.03381, eps: 0.982\n",
      "Episode: 89, Reward: 20.0, avg loss: 0.03947, eps: 0.981\n",
      "Episode: 90, Reward: 185.0, avg loss: 0.03491, eps: 0.979\n",
      "Episode: 91, Reward: 70.0, avg loss: 0.03821, eps: 0.979\n",
      "Episode: 92, Reward: 210.0, avg loss: 0.03636, eps: 0.977\n",
      "Episode: 93, Reward: 60.0, avg loss: 0.03782, eps: 0.976\n",
      "Episode: 94, Reward: 50.0, avg loss: 0.03627, eps: 0.976\n",
      "Episode: 95, Reward: 75.0, avg loss: 0.04083, eps: 0.975\n",
      "Episode: 96, Reward: 65.0, avg loss: 0.04519, eps: 0.974\n",
      "Episode: 97, Reward: 225.0, avg loss: 0.03871, eps: 0.972\n",
      "Episode: 98, Reward: 210.0, avg loss: 0.03739, eps: 0.971\n",
      "Episode: 99, Reward: 55.0, avg loss: 0.04098, eps: 0.970\n",
      "Episode: 100, Reward: 125.0, avg loss: 0.03942, eps: 0.968\n",
      "Episode: 101, Reward: 175.0, avg loss: 0.03987, eps: 0.967\n",
      "Episode: 102, Reward: 120.0, avg loss: 0.03499, eps: 0.966\n",
      "Episode: 103, Reward: 260.0, avg loss: 0.03574, eps: 0.964\n",
      "Episode: 104, Reward: 120.0, avg loss: 0.03777, eps: 0.963\n",
      "Episode: 105, Reward: 240.0, avg loss: 0.03451, eps: 0.961\n",
      "Episode: 106, Reward: 210.0, avg loss: 0.03854, eps: 0.959\n",
      "Episode: 107, Reward: 305.0, avg loss: 0.03750, eps: 0.957\n",
      "Episode: 108, Reward: 155.0, avg loss: 0.03638, eps: 0.956\n",
      "Episode: 109, Reward: 185.0, avg loss: 0.04186, eps: 0.954\n",
      "Episode: 110, Reward: 110.0, avg loss: 0.03618, eps: 0.953\n",
      "Episode: 111, Reward: 75.0, avg loss: 0.03394, eps: 0.952\n",
      "Episode: 112, Reward: 135.0, avg loss: 0.03573, eps: 0.951\n",
      "Episode: 113, Reward: 215.0, avg loss: 0.03402, eps: 0.949\n",
      "Episode: 114, Reward: 80.0, avg loss: 0.03802, eps: 0.948\n",
      "Episode: 115, Reward: 120.0, avg loss: 0.03779, eps: 0.947\n",
      "Episode: 116, Reward: 210.0, avg loss: 0.03741, eps: 0.946\n",
      "Episode: 117, Reward: 230.0, avg loss: 0.03797, eps: 0.945\n",
      "Episode: 118, Reward: 180.0, avg loss: 0.03742, eps: 0.943\n",
      "Episode: 119, Reward: 135.0, avg loss: 0.03726, eps: 0.942\n",
      "Episode: 120, Reward: 330.0, avg loss: 0.04136, eps: 0.940\n",
      "Episode: 121, Reward: 110.0, avg loss: 0.03952, eps: 0.939\n",
      "Episode: 122, Reward: 130.0, avg loss: 0.03665, eps: 0.938\n",
      "Episode: 123, Reward: 40.0, avg loss: 0.03752, eps: 0.936\n",
      "Episode: 124, Reward: 75.0, avg loss: 0.03611, eps: 0.935\n",
      "Episode: 125, Reward: 460.0, avg loss: 0.03803, eps: 0.933\n",
      "Episode: 126, Reward: 210.0, avg loss: 0.03803, eps: 0.932\n",
      "Episode: 127, Reward: 155.0, avg loss: 0.03955, eps: 0.931\n",
      "Episode: 128, Reward: 70.0, avg loss: 0.03592, eps: 0.929\n",
      "Episode: 129, Reward: 110.0, avg loss: 0.04067, eps: 0.929\n",
      "Episode: 130, Reward: 50.0, avg loss: 0.04027, eps: 0.928\n",
      "Episode: 131, Reward: 80.0, avg loss: 0.04024, eps: 0.927\n",
      "Episode: 132, Reward: 105.0, avg loss: 0.03258, eps: 0.926\n",
      "Episode: 133, Reward: 135.0, avg loss: 0.04073, eps: 0.925\n",
      "Episode: 134, Reward: 25.0, avg loss: 0.04110, eps: 0.923\n",
      "Episode: 135, Reward: 110.0, avg loss: 0.04536, eps: 0.922\n",
      "Episode: 136, Reward: 325.0, avg loss: 0.04222, eps: 0.921\n",
      "Episode: 137, Reward: 145.0, avg loss: 0.04380, eps: 0.919\n",
      "Episode: 138, Reward: 35.0, avg loss: 0.04044, eps: 0.919\n",
      "Episode: 139, Reward: 105.0, avg loss: 0.04620, eps: 0.917\n",
      "Episode: 140, Reward: 105.0, avg loss: 0.03867, eps: 0.916\n",
      "Episode: 141, Reward: 430.0, avg loss: 0.04492, eps: 0.914\n",
      "Episode: 142, Reward: 55.0, avg loss: 0.04223, eps: 0.913\n",
      "Episode: 143, Reward: 350.0, avg loss: 0.04435, eps: 0.912\n",
      "Episode: 144, Reward: 120.0, avg loss: 0.04019, eps: 0.910\n",
      "Episode: 145, Reward: 160.0, avg loss: 0.03918, eps: 0.909\n",
      "Episode: 146, Reward: 105.0, avg loss: 0.03937, eps: 0.908\n",
      "Episode: 147, Reward: 265.0, avg loss: 0.04127, eps: 0.906\n",
      "Episode: 148, Reward: 105.0, avg loss: 0.03950, eps: 0.905\n",
      "Episode: 149, Reward: 340.0, avg loss: 0.04036, eps: 0.904\n",
      "Episode: 150, Reward: 230.0, avg loss: 0.03920, eps: 0.902\n",
      "Episode: 151, Reward: 215.0, avg loss: 0.03889, eps: 0.900\n",
      "Episode: 152, Reward: 155.0, avg loss: 0.04468, eps: 0.899\n",
      "Episode: 153, Reward: 210.0, avg loss: 0.04029, eps: 0.898\n",
      "Episode: 154, Reward: 410.0, avg loss: 0.04411, eps: 0.895\n",
      "Episode: 155, Reward: 145.0, avg loss: 0.04887, eps: 0.893\n",
      "Episode: 156, Reward: 65.0, avg loss: 0.05383, eps: 0.893\n",
      "Episode: 157, Reward: 155.0, avg loss: 0.04282, eps: 0.891\n",
      "Episode: 158, Reward: 30.0, avg loss: 0.04471, eps: 0.891\n",
      "Episode: 159, Reward: 135.0, avg loss: 0.04924, eps: 0.890\n",
      "Episode: 160, Reward: 225.0, avg loss: 0.04928, eps: 0.888\n",
      "Episode: 161, Reward: 15.0, avg loss: 0.04912, eps: 0.887\n",
      "Episode: 162, Reward: 110.0, avg loss: 0.04987, eps: 0.886\n",
      "Episode: 163, Reward: 90.0, avg loss: 0.03774, eps: 0.885\n",
      "Episode: 164, Reward: 240.0, avg loss: 0.04196, eps: 0.884\n",
      "Episode: 165, Reward: 160.0, avg loss: 0.03748, eps: 0.882\n",
      "Episode: 166, Reward: 210.0, avg loss: 0.04475, eps: 0.881\n",
      "Episode: 167, Reward: 65.0, avg loss: 0.03498, eps: 0.880\n",
      "Episode: 168, Reward: 300.0, avg loss: 0.04326, eps: 0.878\n",
      "Episode: 169, Reward: 95.0, avg loss: 0.03822, eps: 0.878\n",
      "Episode: 170, Reward: 135.0, avg loss: 0.03751, eps: 0.876\n",
      "Episode: 171, Reward: 225.0, avg loss: 0.04300, eps: 0.874\n",
      "Episode: 172, Reward: 155.0, avg loss: 0.04092, eps: 0.873\n",
      "Episode: 173, Reward: 55.0, avg loss: 0.04262, eps: 0.872\n",
      "Episode: 174, Reward: 155.0, avg loss: 0.05117, eps: 0.871\n",
      "Episode: 175, Reward: 150.0, avg loss: 0.04540, eps: 0.869\n",
      "Episode: 176, Reward: 375.0, avg loss: 0.04357, eps: 0.866\n",
      "Episode: 177, Reward: 20.0, avg loss: 0.04331, eps: 0.865\n",
      "Episode: 178, Reward: 45.0, avg loss: 0.04199, eps: 0.864\n",
      "Episode: 179, Reward: 50.0, avg loss: 0.03948, eps: 0.863\n",
      "Episode: 180, Reward: 525.0, avg loss: 0.04485, eps: 0.861\n",
      "Episode: 181, Reward: 425.0, avg loss: 0.04563, eps: 0.859\n",
      "Episode: 182, Reward: 60.0, avg loss: 0.04201, eps: 0.858\n",
      "Episode: 183, Reward: 75.0, avg loss: 0.04533, eps: 0.857\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import imageio\n",
    "\n",
    "STORE_PATH = 'TensorBoard'\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_MIN_ITER = 500000\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "TAU = 0.08\n",
    "POST_PROCESS_IMAGE_SIZE = (105, 80, 1)\n",
    "DELAY_TRAINING = 50000\n",
    "NUM_FRAMES = 4\n",
    "GIF_RECORDING_FREQ = 100\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v0\")\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "class DQModel(keras.Model):\n",
    "    def __init__(self, hidden_size: int, num_actions: int, dueling: bool):\n",
    "        super(DQModel, self).__init__()\n",
    "        self.dueling = dueling\n",
    "        self.conv1 = keras.layers.Conv2D(16, (8, 8), (4, 4), activation='relu')\n",
    "        self.conv2 = keras.layers.Conv2D(32, (4, 4), (2, 2), activation='relu')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.adv_dense = keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "        self.adv_out = keras.layers.Dense(num_actions,\n",
    "                                          kernel_initializer=keras.initializers.he_normal())\n",
    "        if dueling:\n",
    "            self.v_dense = keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                         kernel_initializer=keras.initializers.he_normal())\n",
    "            self.v_out = keras.layers.Dense(1, kernel_initializer=keras.initializers.he_normal())\n",
    "            self.lambda_layer = keras.layers.Lambda(lambda x: x - tf.reduce_mean(x))\n",
    "            self.combine = keras.layers.Add()\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        adv = self.adv_dense(x)\n",
    "        adv = self.adv_out(adv)\n",
    "        if self.dueling:\n",
    "            v = self.v_dense(x)\n",
    "            v = self.v_out(v)\n",
    "            norm_adv = self.lambda_layer(adv)\n",
    "            combined = self.combine([v, norm_adv])\n",
    "            return combined\n",
    "        return adv\n",
    "\n",
    "primary_network = DQModel(256, num_actions, True)\n",
    "target_network = DQModel(256, num_actions, True)\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "# make target_network = primary_network\n",
    "for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "    t.assign(e)\n",
    "\n",
    "primary_network.compile(optimizer=keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._actions = np.zeros(max_memory, dtype=np.int32)\n",
    "        self._rewards = np.zeros(max_memory, dtype=np.float32)\n",
    "        self._frames = np.zeros((POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], max_memory), dtype=np.float32)\n",
    "        self._terminal = np.zeros(max_memory, dtype=np.bool)\n",
    "        self._i = 0\n",
    "\n",
    "    def add_sample(self, frame, action, reward, terminal):\n",
    "        self._actions[self._i] = action\n",
    "        self._rewards[self._i] = reward\n",
    "        self._frames[:, :, self._i] = frame[:, :, 0]\n",
    "        self._terminal[self._i] = terminal\n",
    "        if self._i % (self._max_memory - 1) == 0 and self._i != 0:\n",
    "            self._i = BATCH_SIZE + NUM_FRAMES + 1\n",
    "        else:\n",
    "            self._i += 1\n",
    "\n",
    "    def sample(self):\n",
    "        if self._i < BATCH_SIZE + NUM_FRAMES + 1:\n",
    "            raise ValueError(\"Not enough memory to extract a batch\")\n",
    "        else:\n",
    "            rand_idxs = np.random.randint(NUM_FRAMES + 1, self._i, size=BATCH_SIZE)\n",
    "            states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            next_states = np.zeros((BATCH_SIZE, POST_PROCESS_IMAGE_SIZE[0], POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES),\n",
    "                             dtype=np.float32)\n",
    "            for i, idx in enumerate(rand_idxs):\n",
    "                states[i] = self._frames[:, :, idx - 1 - NUM_FRAMES:idx - 1]\n",
    "                next_states[i] = self._frames[:, :, idx - NUM_FRAMES:idx]\n",
    "            return states, self._actions[rand_idxs], self._rewards[rand_idxs], next_states, self._terminal[rand_idxs]\n",
    "\n",
    "\n",
    "memory = Memory(500000)\n",
    "# memory = Memory(100)\n",
    "\n",
    "\n",
    "def image_preprocess(image, new_size=(105, 80)):\n",
    "    # convert to greyscale, resize and normalize the image\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.image.resize(image, new_size)\n",
    "    image = image / 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def choose_action(state, primary_network, eps, step):\n",
    "    if step < DELAY_TRAINING:\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        if random.random() < eps:\n",
    "            return random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(primary_network(tf.reshape(state, (1, POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                           POST_PROCESS_IMAGE_SIZE[1], NUM_FRAMES)).numpy()))\n",
    "\n",
    "\n",
    "def update_network(primary_network, target_network):\n",
    "    # update target network parameters slowly from primary network\n",
    "    for t, e in zip(target_network.trainable_variables, primary_network.trainable_variables):\n",
    "        t.assign(t * (1 - TAU) + e * TAU)\n",
    "\n",
    "\n",
    "def process_state_stack(state_stack, state):\n",
    "    for i in range(1, state_stack.shape[-1]):\n",
    "        state_stack[:, :, i - 1].assign(state_stack[:, :, i])\n",
    "    state_stack[:, :, -1].assign(state[:, :, 0])\n",
    "    return state_stack\n",
    "\n",
    "\n",
    "def record_gif(frame_list, episode, fps=50):\n",
    "    imageio.mimsave(STORE_PATH + f\"/SPACE_INVADERS_EPISODE-{episode}.gif\", frame_list, fps=fps) #duration=duration_per_frame)\n",
    "\n",
    "\n",
    "def train(primary_network, memory, target_network=None):\n",
    "    states, actions, rewards, next_states, terminal = memory.sample()\n",
    "    # predict Q(s,a) given the batch of states\n",
    "    prim_qt = primary_network(states)\n",
    "    # predict Q(s',a') from the evaluation network\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    # copy the prim_qt tensor into the target_q tensor - we then will update one index corresponding to the max action\n",
    "    target_q = prim_qt.numpy()\n",
    "    updates = rewards\n",
    "    valid_idxs = terminal != True\n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    if target_network is None:\n",
    "        updates[valid_idxs] += GAMMA * np.amax(prim_qtp1.numpy()[valid_idxs, :], axis=1)\n",
    "    else:\n",
    "        prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "        q_from_target = target_network(next_states)\n",
    "        updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], prim_action_tp1[valid_idxs]]\n",
    "    target_q[batch_idxs, actions] = updates\n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss\n",
    "\n",
    "num_episodes = 1000000\n",
    "eps = MAX_EPSILON\n",
    "render = False\n",
    "train_writer = tf.summary.create_file_writer(STORE_PATH + f\"/DuelingQSI_{dt.datetime.now().strftime('%d%m%Y%H%M')}\")\n",
    "double_q = True\n",
    "steps = 0\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = image_preprocess(state)\n",
    "    state_stack = tf.Variable(np.repeat(state.numpy(), NUM_FRAMES).reshape((POST_PROCESS_IMAGE_SIZE[0],\n",
    "                                                                            POST_PROCESS_IMAGE_SIZE[1],\n",
    "                                                                            NUM_FRAMES)))\n",
    "    cnt = 1\n",
    "    avg_loss = 0\n",
    "    tot_reward = 0\n",
    "    if i % GIF_RECORDING_FREQ == 0:\n",
    "        frame_list = []\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = choose_action(state_stack, primary_network, eps, steps)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        tot_reward += reward\n",
    "        if i % GIF_RECORDING_FREQ == 0:\n",
    "            frame_list.append(tf.cast(tf.image.resize(next_state, (480, 320)), tf.uint8).numpy())\n",
    "        next_state = image_preprocess(next_state)\n",
    "        state_stack = process_state_stack(state_stack, next_state)\n",
    "        # store in memory\n",
    "        memory.add_sample(next_state, action, reward, done)\n",
    "\n",
    "        if steps > DELAY_TRAINING:\n",
    "            loss = train(primary_network, memory, target_network if double_q else None)\n",
    "            update_network(primary_network, target_network)\n",
    "        else:\n",
    "            loss = -1\n",
    "        avg_loss += loss\n",
    "\n",
    "        # linearly decay the eps value\n",
    "        if steps > DELAY_TRAINING:\n",
    "            eps = MAX_EPSILON - ((steps - DELAY_TRAINING) / EPSILON_MIN_ITER) * \\\n",
    "                  (MAX_EPSILON - MIN_EPSILON) if steps < EPSILON_MIN_ITER else \\\n",
    "                MIN_EPSILON\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            if steps > DELAY_TRAINING:\n",
    "                avg_loss /= cnt\n",
    "                print(f\"Episode: {i}, Reward: {tot_reward}, avg loss: {avg_loss:.5f}, eps: {eps:.3f}\")\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.scalar('reward', tot_reward, step=i)\n",
    "                    tf.summary.scalar('avg loss', avg_loss, step=i)\n",
    "            else:\n",
    "                print(f\"Pre-training...Episode: {i}\\r\", end=\"\")\n",
    "            if i % GIF_RECORDING_FREQ == 0:\n",
    "                record_gif(frame_list, i)\n",
    "            break\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
